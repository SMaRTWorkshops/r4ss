{
  "hash": "0501a13b55ac4c29676e2f99937dc829",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: ../styles.css\n    slide-number: true\n    show-slide-number: all\n    preview-links: auto\n    self-contained: true\n    progress: true\n    history: true\n    hash-type: number\n    theme: default\n    code-block-background: true\n    highlight-style: zenburn\n    code-link: false\n    code-copy: true\n    pagetitle: \"R4SS Day 3B\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2022-07-27\"\n---\n\n\n::: {.my-title}\n# [Introduction to R]{.blue} <br />for Social Scientists\n\n::: {.my-grey}\n[Workshop Day 3B | 2022-07-27]{}<br />\n[Jeffrey M. Girard | Pitt Methods]{}\n:::\n\n![](../img/proud_coder_357EDD.svg){.absolute bottom=0 right=0 width=400}\n:::\n\n<!-- Model II -->\n\n# Model II \n\n## Basic Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   [Basic regression]{.b .blue} predicts one variable $y$ from another variable $x$ using a straight line\n\n::: {.fragment .mt1}\n-   This line is defined by two parameters\n    -   The [intercept]{.b .green} is the value of $y$ when $x=0$\n    -   The [slope]{.b .green} is the change in $y$ expected for a change of 1 in $x$ (from $x=0$ to $x=1$)\n:::\n\n::: {.fragment .mt1}\n-   We will use `lm()` to fit regression models\n    -   This will solve using ordinary least squares\n    -   We need to give it the [data]{.b .green} and a [formula]{.b .green}\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li ogfgksuz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Basic Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(datawizard)\nlibrary(parameters)\nlibrary(performance)\nlibrary(effects)\nlibrary(ggeffects)\n\nyearspubs <- read_csv(\"yearspubs.csv\")\nyearspubs\n\n# ==============================================================================\n\n# LESSON: Regression is a special case of the linear model, so we use lm()\n\nfit <- lm(\n  formula = salary ~ yrs_since,\n  data = yearspubs\n)\n\n# ==============================================================================\n\n# TIP: Get a parameter summary using model_parameters() from {parameters}\n\nfit\n\nmodel_parameters(fit)\n\n# ==============================================================================\n\n# TIP: Get effect sizes (standardized results) using standardize = \"refit\"\n\nmodel_parameters(fit, standardize = \"refit\")\n\n# ==============================================================================\n\n# TIP: Get a performance summary using model_performance() from {performance}\n\nmodel_performance(fit)\n\n# ==============================================================================\n\n# TIP: Visualize the model's predictions using ggeffect() from {ggeffects}\n\nggeffect(fit, terms = \"yrs_since\") |> plot()\n\n# ==============================================================================\n\n# LESSON: To center the predictor, use mutate() and center() from {datawizard}\n\nyearspubs <- mutate(yearspubs, yrs_since_c = center(yrs_since))\n\nfit_c <- lm(\n  formula = salary ~ yrs_since_c,\n  data = yearspubs\n)\n\nmodel_parameters(fit_c)\n\n# NOTE: Centering will change the intercept in basic regression (not the slope)\n```\n:::\n\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We can also include [multiple predictors]{.b .blue} to assess the [partial effect]{.b .green} of each predictor\n    -   This allows us to account for the variance shared by the predictors and the outcome\n\n::: {.fragment .mt1}\n-   This changes the slopes' interpretations\n    -   The slope of $x_1$ is no longer just the change in $y$ expected for a change of 1 in $x_1$\n    -   It is now the change in $y$ expected for a change of 1 in $x_1$ **when controlling for $x_2$**\n    -   The slope of $x_2$ similarly controls for $x_1$\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li jmkpuued trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"45%\"}\n- In the model $y \\sim x_1$\n    -   The $x_1$ slope captures [b]{.b .green} + [c]{.b .green}\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_2$\n    -   The $x_2$ slope captures [c]{.b .green} + [f]{.b .green}\n:::\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_1 + x_2$\n    -   The $x_1$ slope captures [b]{.b .green} only\n    -   The $x_2$ slope captures [f]{.b .green} only\n    -   So the overlap of [c]{.b .green} is removed\n:::\n\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"45%\"}\n![](../img/venn.png)\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Multiple Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(parameters)\nlibrary(performance)\nlibrary(ggeffects)\n\nyearspubs <- read_csv(\"../data/yearspubs.csv\")\nyearspubs\n\n# ==============================================================================\n\n# LESSON: To add more predictors to the formula, just separate them by +\n\nfit3 <- lm(\n  formula = salary ~ yrs_since + n_pubs,\n  data = yearspubs\n)\n\nmodel_parameters(fit3)\n\nmodel_performance(fit3)\n\nmodel_parameters(fit3, standardize = \"refit\")\n\n# ==============================================================================\n\n# LESSON: Our effect plots are now \"marginalized\" across the other variable\n\nggeffect(fit3, terms = \"yrs_since\") |> plot()\n\nggeffect(fit3, terms = \"n_pubs\") |> plot()\n\n# ==============================================================================\n\n# USECASE: We can compare our models in terms of parameters and performance\n\nfit1 <- lm(salary ~ yrs_since, data = yearspubs)\nfit2 <- lm(salary ~ n_pubs, data = yearspubs)\n\ncompare_parameters(fit1, fit2, fit3)\n\ncompare_performance(fit1, fit2, fit3)\n```\n:::\n\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Categorical Predictors {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   To include categorical predictors in regression models, we can use [dummy coding]{.b .blue}\n    -   This creates binary predictor variables\n\n::: {.fragment .mt1}\n-   One [reference group]{.b .green} does not get a slope\n    -   Instead, it controls the model intercept\n    -   All other groups' slopes are just deviations from the intercept\n:::\n\n::: {.fragment .mt1}\n-   There is no need to create dummy codes in R\n    -   Just include a [factor]{.b .green} as a predictor variable\n    -   The **first level** will be the reference group\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li cdbgwqyw trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Categorical Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(parameters)\nlibrary(performance)\nlibrary(ggeffects)\n\nsalaries <- read_csv(\"salaries.csv\")\nsalaries\n\n# ==============================================================================\n\n# USECASE: Compare three groups with regression (instead of oneway anova)\n\nsalaries <- \n  salaries |> \n  mutate(\n    rank = factor(\n      rank, \n      levels = c(\"Assistant\", \"Associate\", \"Full\") # put Assistant first\n    )\n  )\n\nfit <- lm(salary ~ rank, data = salaries)\n\nmodel_parameters(fit) # estimate intercept and slopes\n\nmodel_parameters(fit, standardize = \"refit\") # estimate standardized effects\n\nmodel_performance(fit) # estimate model performance\n\nggeffect(fit, terms = \"rank\") # estimate group means for rank\n\nggeffect(fit, terms = \"rank\") |> plot() # plot group means for rank\n\nfit |> aov() |> model_parameters() # recreate ANOVA-style F-test\n\n# ==============================================================================\n\n# USECASE: Change the reference group to the Associate rank\n\nsalaries2 <- \n  salaries |> \n  mutate(\n    rank = factor(\n      rank, \n      levels = c(\"Associate\", \"Assistant\", \"Full\") # put Associate first\n    )\n  )\n\nfit2 <- lm(salary ~ rank, data = salaries2)\n\nmodel_parameters(fit2) # now the intercept is the Associate rank's mean\n\nggeffect(fit2, terms = \"rank\") # these estimates are identical to before\n\n# ==============================================================================\n\n# USECASE: Regression can even mix categorical and continuous predictors!\n\nfit3 <- lm(salary ~ rank + yrs.since.phd, data = salaries)\n\nmodel_parameters(fit3)\n\n# NOTE: The intercept is now the mean of Assistant rank with 0 years since PhD\n\nmodel_parameters(fit3, standardize = \"refit\")\n\nmodel_performance(fit3)\n\nggeffect(fit3, terms = c(\"yrs.since.phd\", \"rank\")) |> plot()\n```\n:::\n\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Interaction Effects {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We may want to know if the effect of one predictor [depends on]{.b .green} the value on another predictor\n    -   Does the effect of hours of **exercise** on weight loss *depend on* biological **sex**?\n    -   Does the effect of hours of **exercise** on weight loss *depend on* the **effort** put in?\n\n::: {.fragment .mt1}\n-   To answer these, we can test [interaction effects]{.b .blue}\n    -   Interaction effects are just slopes for the [product]{.b .green} of two or more predictors\n    -   We can include continuous and categorical predictors in interaction effects\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li nmlpnruz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Interactions Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(parameters)\nlibrary(performance)\nlibrary(ggeffects)\n\nexercise <- read_csv(\"exercise.csv\")\nexercise\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit1 <- lm(loss ~ hours + sex, data = exercise)\n\nmodel_parameters(fit1)\n\nggeffect(fit1, terms = c(\"hours\", \"sex\")) |> plot()\n\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on sex (and vice versa)?\n\nfit2 <- lm(loss ~ hours * sex, data = exercise)\n\nmodel_parameters(fit2)\n\nggeffect(fit2, terms = c(\"hours\", \"sex\")) |> plot()\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit3 <- lm(loss ~ hours + effort, data = exercise)\n\nmodel_parameters(fit3)\n\nggeffect(fit3, terms = c(\"hours\", \"effort\")) |> plot()\n\n# NOTE: A model with no interaction will have parallel lines\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on effort (and vice versa)?\n\nfit4 <- lm(loss ~ hours * effort, data = exercise)\n\nmodel_parameters(fit4)\n\nggeffect(fit4, terms = c(\"hours\", \"effort\")) |> plot() # put hours on x-axis\n\nggeffect(fit4, terms = c(\"effort\", \"hours\")) |> plot() # put effort on x-axis\n\n# ==============================================================================\n\n# LESSON: Be cautious about higher-level interactions\n\nfit5 <- lm(loss ~ hours * effort * sex, data = exercise)\n\nmodel_parameters(fit5)\n\nggeffect(fit5, terms = c(\"hours\", \"effort\", \"sex\")) |> plot()\n```\n:::\n\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## A Formula Resource {.smaller}\n\n::: {.pv4}\n\n<table width=\"100%\">\n<tr>\n  <th>Formula</th>\n  <th colspan=7>Slopes Estimated</th>\n</tr>\n<tr>\n  <td width=\"30%\">`y ~ x`</td>\n  <td width=\"10%\">$x$</td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n</tr>\n<tr>\n  <td>`y ~ x + w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x + w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * (w + z)`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w * z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td>$wz$</td>\n  <td>$xwz$</td>\n</tr>\n</table>\n\n::: {.pv4}\nNote that predictors can be continuous (i.e., numbers) or categorical (i.e., factors), but if a categorical predictor has more than two levels, you will end up with additional slopes.\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Regression Diagnostics {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   LM makes some [assumptions]{.b .green} about the data\n    -   If violated, this can lead to...\n    -   Biased [coefficient]{.b} estimates\n    -   Biased [standard error]{.b} estimates\n\n::: {.fragment .mt1}\n-   [Regression Diagnostics]{.b .blue} check for...\n    -   Violated assumptions\n    -   Outlier observations\n    -   Multicollinearity\n    -   Prediction quality\n:::\n\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li oamdefle trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n::: footer\n\\[3B\\] Model II\n:::\n\n## Diagnostics Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfit <- lm(salary ~ yrs.since.phd + yrs.service, data = salaries)\n\n# ==============================================================================\n\n# USECASE: Check if the residuals appear normally distributed\n\nfit |> check_normality()\n\nfit |> check_normality() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if the residuals show heteroscedasticity (constant variance)\n\nfit |> check_heteroscedasticity()\n\nfit |> check_heteroscedasticity() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if the model's predictions match the observed distribution\n\nfit |> check_predictions()\n\n# ==============================================================================\n\n# USECASE: Check if any of the predictors show collinearity with others\n\nfit |> check_collinearity()\n\nfit |> check_collinearity() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if any of the observations would be considered outliers\n\nfit |> check_outliers()\n\nfit |> check_outliers() |> plot()\n```\n:::\n\n\n::: footer\n\\[3B\\] Model II\n:::\n\n<!-- Practice V -->\n\n# [Practice V](https://pittmethods.github.io/r4ss/Day_3/Day3B_Practice.html){preview-link=\"false\"}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}